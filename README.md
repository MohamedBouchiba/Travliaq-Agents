# Travliaq-Agents

**Pipeline CrewAI intelligente pour analyser les questionnaires voyage et gÃ©nÃ©rer des spÃ©cifications de trip structurÃ©es.**

Utilise des agents IA multi-rÃ´les (analyste, challenger, architecte) avec outputs validÃ©s par Pydantic, observabilitÃ© complÃ¨te et intÃ©gration MCP.

---

## ğŸš€ DÃ©marrage Rapide

### 1. Installation

```bash
# Cloner le projet
git clone git@github.com:MohamedBouchiba/Travliaq-Agents.git
cd Travliaq-Agents

# CrÃ©er et activer l'environnement virtuel
python -m venv .venv
source .venv/Scripts/activate  # Windows Git Bash
# ou: .venv\Scripts\activate   # Windows CMD
# ou: source .venv/bin/activate # Linux/macOS

# Installer les dÃ©pendances
pip install -r requirements.txt

# Configurer les variables d'environnement
# Ã‰diter .env et ajouter votre OPENAI_API_KEY
```

---

## ğŸ“‹ MÃ©thodes d'ExÃ©cution

### Option 1: API FastAPI (Production)

DÃ©marrer le serveur API :

```bash
# MÃ©thode 1 - Script Python
python run.py

# MÃ©thode 2 - Uvicorn direct
uvicorn app.api.main:app --reload --host 0.0.0.0 --port 8000
```

L'API sera accessible sur **http://localhost:8000**

- Documentation: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

**Appeler la pipeline via l'API** :

```bash
# Health check
curl http://localhost:8000/api/v1/health

# ExÃ©cuter la pipeline avec un questionnaire-id
curl -X POST "http://localhost:8000/api/v1/questionnaire" \
  -H "Content-Type: application/json" \
  -d '{"questionnaire_id": "c786404a-18ae-4a1f-b8a1-403a3de78540"}'
```

---

### Option 2: CLI avec ID Questionnaire

ExÃ©cuter la pipeline directement depuis la ligne de commande :

```bash
# Avec un ID questionnaire depuis Supabase
python crew_pipeline_cli.py --questionnaire-id c786404a-18ae-4a1f-b8a1-403a3de78540

# Avec un fichier JSON local
python crew_pipeline_cli.py --input-file examples/traveller_persona_input.json

# Forcer un modÃ¨le spÃ©cifique
python crew_pipeline_cli.py \
  --questionnaire-id c786404a-18ae-4a1f-b8a1-403a3de78540 \
  --llm-provider openai \
  --model gpt-4o-mini
```

---

### Option 3: Script avec ID PrÃ©-configurÃ©

**MÃ©thode la plus simple pour tester** :

1. Ã‰diter le fichier de test pour dÃ©finir l'ID :

```python
# Dans examples/test_pipeline.py ou crÃ©er un nouveau script
from app.crew_pipeline.pipeline import run_pipeline_from_payload
import json

# Charger un exemple ou dÃ©finir un payload
with open('examples/traveller_persona_input.json') as f:
    payload = json.load(f)

# ExÃ©cuter la pipeline
result = run_pipeline_from_payload(payload)
print(json.dumps(result, indent=2))
```

2. ExÃ©cuter le script :

```bash
python examples/test_pipeline.py
```

---

## ğŸ“Š Outputs GÃ©nÃ©rÃ©s

Chaque exÃ©cution crÃ©e un dossier `output/<run_id>/` contenant :

```
output/
â””â”€â”€ <run_id>/
    â”œâ”€â”€ run_output.json          # RÃ©sultat final enrichi
    â”œâ”€â”€ metrics.json              # MÃ©triques de performance (durÃ©e, tokens, coÃ»ts)
    â””â”€â”€ tasks/                    # Outputs par tÃ¢che
        â”œâ”€â”€ traveller_profile_brief.json
        â”œâ”€â”€ persona_challenge_review.json
        â””â”€â”€ trip_specifications_design.json
```

**MÃ©triques collectÃ©es** :

- â±ï¸ DurÃ©e d'exÃ©cution (totale et par agent)
- ğŸ”¢ Nombre de tokens utilisÃ©s
- ğŸ’° CoÃ»t estimÃ© (USD)
- ğŸ“Š Scores de qualitÃ© des outputs

---

## ğŸ—ï¸ Architecture

```
Questionnaire + Persona Inference
           â†“
    Agent 1: Analyste (PersonaAnalysisOutput)
           â†“
    Agent 2: Challenger (PersonaChallengeOutput)
           â†“
    Agent 3: Architecte (TripSpecificationsOutput)
           â†“
    Trip Request NormalisÃ© + MÃ©triques
```

**Best Practices AppliquÃ©es** :

- âœ… Outputs structurÃ©s avec Pydantic
- âœ… ObservabilitÃ© et mÃ©triques complÃ¨tes
- âœ… Retry logic et timeouts pour outils MCP
- âœ… Optimisation LLM (max_iter, memory)
- âœ… Tests unitaires automatisÃ©s

---

## ğŸ“š Documentation DÃ©taillÃ©e

- **[Pipeline Workflow](documentation/trip_generation_workflow.md)** - Flux dÃ©taillÃ© de la pipeline
- **[Configuration](app/crew_pipeline/config/)** - Agents, tÃ¢ches et crew
- **[Best Practices](C:/Users/User/.gemini/antigravity/brain/07d5ff8c-fec0-4cb9-9ee0-0746393a7ee4/implementation_plan.md)** - Plan d'implÃ©mentation
- **[Walkthrough](C:/Users/User/.gemini/antigravity/brain/07d5ff8c-fec0-4cb9-9ee0-0746393a7ee4/walkthrough.md)** - Modifications apportÃ©es

---

## ğŸ§ª Tests

```bash
# Tests unitaires des modÃ¨les
pytest tests/test_models.py -v

# Tests complets
pytest tests/ -v

# Coverage
pytest tests/ --cov=app --cov-report=html
```

---

## ğŸ”§ Configuration

### Variables d'Environnement (.env)

```env
# LLM Provider
LLM_PROVIDER=openai
OPENAI_API_KEY=your_key_here
MODEL=gpt-4o-mini

# Supabase
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=your_supabase_key
POSTGRES_HOST=your_postgres_host
POSTGRES_DB=postgres
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_password

# MCP Server
MCP_SERVER_URL=https://travliaq-mcp-production.up.railway.app/mcp

# CrewAI
CREW_OUTPUT_DIR=output
VERBOSE=true
```

---

## ğŸ› Troubleshooting

### ModuleNotFoundError: No module named 'X'

```bash
# RÃ©installer les dÃ©pendances dans le venv
source .venv/Scripts/activate  # Activer le venv
pip install -r requirements.txt
```

### Erreur psycopg2

```bash
# Si .venv corrompu, le recrÃ©er
rm -rf .venv
python -m venv .venv
source .venv/Scripts/activate
pip install -r requirements.txt
```

### Port 8000 dÃ©jÃ  utilisÃ©

Modifier dans `.env` :

```env
API_PORT=8001
```

---

## ğŸ“¦ Structure du Projet

```
Travliaq-Agents/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/                      # API FastAPI
â”‚   â”œâ”€â”€ crew_pipeline/            # Pipeline CrewAI
â”‚   â”‚   â”œâ”€â”€ config/               # Agents, tÃ¢ches, crew (YAML)
â”‚   â”‚   â”œâ”€â”€ models.py             # ModÃ¨les Pydantic
â”‚   â”‚   â”œâ”€â”€ observability.py      # MÃ©triques & monitoring
â”‚   â”‚   â”œâ”€â”€ pipeline.py           # Orchestration principale
â”‚   â”‚   â””â”€â”€ mcp_tools.py          # Outils MCP
â”‚   â””â”€â”€ services/                 # Services (Supabase)
â”œâ”€â”€ tests/                        # Tests unitaires
â”œâ”€â”€ examples/                     # Exemples et fixtures
â”œâ”€â”€ output/                       # Outputs gÃ©nÃ©rÃ©s
â”œâ”€â”€ documentation/                # Documentation
â”œâ”€â”€ crew_pipeline_cli.py          # CLI principal
â””â”€â”€ run.py                        # Launcher API
```

---

## ğŸ¤ Contact & Support

- **Repository**: [Travliaq-Agents](https://github.com/MohamedBouchiba/Travliaq-Agents)
- **Documentation**: Voir `documentation/`
- **Issues**: GitHub Issues

## Architecture

```
API Endpoint â†’ PostgreSQL (Supabase) â†’ RÃ©cupÃ©ration Questionnaire â†’ CrewAI Pipeline â†’ JSON Trip
```

## Installation

### 1. Cloner le projet

```bash
git clone git@github.com:MohamedBouchiba/Travliaq-Agents.git
cd Travliaq-Agents
```

### 2. CrÃ©er un environnement virtuel

**Windows:**

```bash
python -m venv .venv
.venv\Scripts\activate
```

**Linux/macOS:**

```bash
python3 -m venv .venv
source .venv/bin/activate
```

### 3. Installer les dÃ©pendances

**MÃ©thode automatique:**

Windows:

```bash
install.bat
```

Linux/macOS:

```bash
chmod +x install.sh
./install.sh

# Ou avec make
make install
```

**MÃ©thode manuelle:**

```bash
pip install -r requirements.txt
```

### 4. VÃ©rifier la configuration

Le fichier `.env` est dÃ©jÃ  configurÃ© avec les credentials Supabase.

> â„¹ï¸ Remplacez la valeur `OPENAI_API_KEY` ou laissez-la vide si vous
> prÃ©fÃ©rez fournir la clÃ© via une variable d'environnement (par exemple
> `set OPENAI_API_KEY=...` sous Windows ou `export OPENAI_API_KEY=...` sur
> macOS/Linux). Les valeurs factices comme `your_key_here` sont ignorÃ©es
> automatiquement afin de privilÃ©gier les clÃ©s rÃ©ellement dÃ©finies.

```bash
cat .env  # Linux/macOS
type .env # Windows
```

## DÃ©marrage

### Lancer l'API

**MÃ©thode 1 - Script automatique (recommandÃ©):**

Windows:

```bash
start.bat
# Ou double-clic sur start.bat
```

Linux/macOS:

```bash
./start.sh

# Ou avec make
make run
```

**MÃ©thode 2 - Script Python (toutes plateformes):**

```bash
python run.py      # Windows
python3 run.py     # Linux/macOS
```

**MÃ©thode 3 - Uvicorn directement:**

```bash
uvicorn app.api.main:app --reload --host 0.0.0.0 --port 8000
```

L'API sera accessible sur **http://localhost:8000**

### Affichage au dÃ©marrage

```
============================================================
ğŸš€ Travliaq-Agents API
============================================================
ğŸ“ URL locale:        http://localhost:8000
ğŸ“š Documentation:     http://localhost:8000/docs
ğŸ“– ReDoc:             http://localhost:8000/redoc
ğŸ’» SystÃ¨me:           Windows (AMD64)
ğŸ Python:            3.11.0
============================================================

ğŸ’¡ ArrÃªter: Ctrl+C
```

### Documentation interactive

Une fois l'API lancÃ©e, accÃ©dez Ã :

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## Utilisation

### 1. Health Check

VÃ©rifier que l'API et la base de donnÃ©es sont accessibles:

```bash
curl http://localhost:8000/api/v1/health
```

**RÃ©ponse:**

```json
{
  "status": "ok",
  "message": "Service is healthy"
}
```

### 2. RÃ©cupÃ©rer un questionnaire (POST)

```bash
curl -X POST "http://localhost:8000/api/v1/questionnaire" \
  -H "Content-Type: application/json" \
  -d '{"questionnaire_id": "c92a18b0-c2d4-4903-abdb-6e7669eb0633"}'
```

### 3. RÃ©cupÃ©rer un questionnaire (GET)

```bash
curl http://localhost:8000/api/v1/questionnaire/c92a18b0-c2d4-4903-abdb-6e7669eb0633
```

**RÃ©ponse:**

```json
{
  "status": "ok",
  "questionnaire_id": "c92a18b0-c2d4-4903-abdb-6e7669eb0633",
  "data": {
    "id": "c92a18b0-c2d4-4903-abdb-6e7669eb0633",
    "email": "theool.milioni@gmail.com",
    "groupe_voyage": "duo",
    "nombre_voyageurs": 2,
    "destination": "Tokyo",
    ...
  }
}
```

### 4. ExÃ©cuter la pipeline CrewAI manuellement

Pour lancer la pipeline en ligne de commande sans passer par l'API :

```bash
python crew_pipeline_cli.py --input-file examples/traveller_persona_input.json
```

ou Ã  partir d'un identifiant de questionnaire :

```bash
python crew_pipeline_cli.py --questionnaire-id <UUID>
```

Vous pouvez Ã©galement forcer dynamiquement le provider et le modÃ¨le utilisÃ©s par
les agents CrewAI sans modifier la configuration globaleÂ :

```bash
python crew_pipeline_cli.py \
  --input-file examples/traveller_persona_input.json \
  --llm-provider openai \
  --model gpt-4.1-mini
```

La pipeline instancie dÃ©sormais deux agents complÃ©mentairesÂ : un architecte
d'insights qui produit l'analyse primaire et un challenger de type ChatGPT qui
raisonne explicitement avant de valider ou d'amender la premiÃ¨re proposition.

> ğŸ’¡ L'ancien raccourci (`python -m app.crew_pipeline`) reste disponible si le dossier
> du projet se trouve dans votre `PYTHONPATH` (par exemple en exÃ©cutant la commande
> depuis la racine du dÃ©pÃ´t).

## Test Rapide

Un script de test est fourni:

```bash
python test_api.py
```

Ce script teste:

- âœ… Health check
- âœ… RÃ©cupÃ©ration via POST
- âœ… RÃ©cupÃ©ration via GET

## Structure du Projet

```
Travliaq-Agents/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py           # Point d'entrÃ©e FastAPI
â”‚   â”‚   â””â”€â”€ routes.py         # Endpoints API
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ supabase_service.py  # Service PostgreSQL
â”‚   â”œâ”€â”€ crew_pipeline/        # Pipeline CrewAI (Ã  venir)
â”‚   â””â”€â”€ config.py             # Configuration centralisÃ©e
â”œâ”€â”€ crew/
â”‚   â”œâ”€â”€ agents.yaml           # DÃ©finition des agents
â”‚   â”œâ”€â”€ tasks.yaml            # DÃ©finition des tÃ¢ches
â”‚   â”œâ”€â”€ crew.yaml             # Configuration CrewAI
â”‚   â””â”€â”€ tools.yaml            # Outils (vides pour l'instant)
â”œâ”€â”€ tests/                    # Tests unitaires
â”œâ”€â”€ output/                   # JSON gÃ©nÃ©rÃ©s
â”œâ”€â”€ .env                      # Configuration (avec credentials)
â”œâ”€â”€ requirements.txt          # DÃ©pendances Python
â””â”€â”€ test_api.py              # Script de test rapide
```

## Endpoints Disponibles

| MÃ©thode | Endpoint                     | Description                          |
| ------- | ---------------------------- | ------------------------------------ |
| GET     | `/`                          | Informations de base                 |
| GET     | `/api/v1/health`             | Health check                         |
| POST    | `/api/v1/questionnaire`      | RÃ©cupÃ©rer questionnaire (body JSON)  |
| GET     | `/api/v1/questionnaire/{id}` | RÃ©cupÃ©rer questionnaire (path param) |

## Prochaines Ã‰tapes

1. âœ… API fonctionnelle avec rÃ©cupÃ©ration des questionnaires
2. âœ… Pipeline CrewAI pour gÃ©nÃ©rer les trips
3. âœ… Outputs structurÃ©s avec Pydantic
4. âœ… ObservabilitÃ© et mÃ©triques de performance
5. ğŸ”œ Stockage des trips gÃ©nÃ©rÃ©s dans Supabase
6. ğŸ”œ Tests unitaires complets

## Best Practices AppliquÃ©es

### Outputs StructurÃ©s avec Pydantic

La pipeline utilise des modÃ¨les Pydantic pour garantir la qualitÃ© et la cohÃ©rence des outputs :

```python
from app.crew_pipeline.models import PersonaAnalysisOutput, PersonaChallengeOutput

# Les agents produisent automatiquement des outputs validÃ©s
# DÃ©finis dans agents.yaml :
# output_pydantic: app.crew_pipeline.models.PersonaAnalysisOutput
```

**Avantages** :

- Validation automatique des donnÃ©es
- Typage fort et autocomplete dans l'IDE
- Documentation intÃ©grÃ©e des schÃ©mas
- DÃ©tection prÃ©coce des erreurs

### ObservabilitÃ© et MÃ©triques

Chaque exÃ©cution de la pipeline gÃ©nÃ¨re des mÃ©triques dÃ©taillÃ©es :

```bash
# Les mÃ©triques sont sauvegardÃ©es dans output/<run_id>/metrics.json
cat output/mon-run-abc123/metrics.json
```

**MÃ©triques collectÃ©es** :

- DurÃ©e d'exÃ©cution totale et par agent
- Nombre de tokens utilisÃ©s
- CoÃ»t estimÃ© (USD)
- Scores de qualitÃ© des outputs
- Erreurs et avertissements

### Gestion d'Erreurs Robuste

Les outils MCP incluent retry logic et timeout :

```yaml
# Configuration dans mcp_tools.py
MCP_TIMEOUT_SECONDS = 30  # Timeout par dÃ©faut
MCP_MAX_RETRIES = 3       # Nombre de tentatives
```

### Optimisations LLM

Configuration optimisÃ©e des agents pour contrÃ´ler les coÃ»ts :

```yaml
# Dans agents.yaml
max_iter: 15 # Limite d'itÃ©rations
memory: true # MÃ©moire contextuelle
reasoning: true # Raisonnement explicite
max_reasoning_attempts: 3 # Tentatives de raisonnement
```

## Logs

L'API affiche des logs dÃ©taillÃ©s:

```
2025-11-14 16:30:00 - INFO - ğŸš€ DÃ©marrage de Travliaq-Agents API
2025-11-14 16:30:00 - INFO - ğŸ“Š Log level: INFO
2025-11-14 16:30:00 - INFO - ğŸ”— Supabase URL: https://cinbnmlfpffmyjmkwbco.supabase.co
2025-11-14 16:30:00 - INFO - ğŸ—„ï¸  PostgreSQL: db.cinbnmlfpffmyjmkwbco.supabase.co:5432
2025-11-14 16:30:05 - INFO - ğŸ“¥ RequÃªte reÃ§ue pour questionnaire: c92a18b0-...
2025-11-14 16:30:05 - INFO - âœ… Connexion PostgreSQL Ã©tablie
2025-11-14 16:30:05 - INFO - âœ… Questionnaire rÃ©cupÃ©rÃ©: c92a18b0-...
```

## Troubleshooting

### ModuleNotFoundError: No module named 'app'

Si tu obtiens cette erreur, **n'utilise PAS** `python app/api/main.py` directement.

**Utilise plutÃ´t:**

```bash
# Option recommandÃ©e
python run.py

# Ou
start.bat

# Ou
uvicorn app.api.main:app --reload
```

### Erreur de connexion PostgreSQL

Si vous obtenez une erreur de connexion:

```
âŒ Erreur connexion PostgreSQL
```

VÃ©rifiez:

1. Les credentials dans `.env`
2. Votre connexion internet
3. Les rÃ¨gles firewall Supabase

### Port 8000 dÃ©jÃ  utilisÃ©

Si le port 8000 est dÃ©jÃ  pris, modifiez dans `.env`:

```
API_PORT=8001
```
